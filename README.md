# AI College Chatbot

An AI-powered intelligent chatbot system designed to automatically answer queries about Sri Satya Sai Group of Colleges () using Natural Language Processing (NLP) and Machine Learning (ML).

## ğŸ“‹ Project Overview

The chatbot system:
- **Extracts** official data from  website () via automated web scraping
- **Processes** raw text using NLP techniques (tokenization, lemmatization, preprocessing)
- **Trains** ML intent classification model using TF-IDF vectorization and Naive Bayes
- **Predicts** user intent and provides confident responses with fallback mechanisms
- **Logs** conversations and low-confidence queries for continuous improvement
- **Provides** interactive web interface (Streamlit) and REST API (FastAPI)

## ğŸš€ Features

âœ¨ **Smart Intent Classification** - Identifies user intent with confidence scoring  
ğŸ›¡ï¸ **Confidence-Based Fallback** - Safe responses when uncertain  
ğŸ“Š **Chat Analytics** - Track conversations and performance metrics  
ğŸ”„ **Continuous Learning** - Low-confidence queries logged for retraining  
âš¡ **Fast API** - RESTful endpoints for easy integration  
ğŸ¨ **Modern UI** - Interactive Streamlit interface  
ğŸ” **Secure** - Input validation and error handling  

## ğŸ“¦ Installation

### Requirements
- **Python 3.9+**
- **Windows OS** (tested on Windows 10+)
- **~500MB disk space** for dependencies

### Step 1: Clone and Navigate
```bash
cd c:\laragon\www\Chaton
```

### Step 2: Create Virtual Environment (Optional but Recommended)
```bash
python -m venv venv
venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

All required packages will be installed:
- **Backend**: FastAPI, Uvicorn, Pydantic
- **Frontend**: Streamlit
- **ML/NLP**: Scikit-learn, Pandas, NumPy, BeautifulSoup4
- **Testing**: Pytest, Pytest-asyncio
- **Utilities**: Requests, Python-dotenv, Werkzeug

## ğŸ”§ Configuration

Configuration is managed via `backend/config.py`:

```python
CONFIDENCE_THRESHOLD = 0.5      # Minimum confidence for KB response (default: 50%)
MIN_CONFIDENCE = 0.3            # Minimum acceptable confidence
MAX_CONFIDENCE = 1.0            # Maximum confidence score
LOG_LEVEL = "INFO"              # Logging level (DEBUG, INFO, WARNING, ERROR)
DEBUG = False                   # Debug mode toggle
```

Environment variables can override defaults via `.env` file:
```
CONFIDENCE_THRESHOLD=0.6
LOG_LEVEL=DEBUG
```

## ğŸ“‚ Project Structure

```
Chaton/
â”œâ”€â”€ backend/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py                  # Application entry point
â”‚   â”œâ”€â”€ config.py                # Configuration settings
â”‚   â”œâ”€â”€ database.py              # SQLite setup and logging
â”‚   â”œâ”€â”€ api/                     # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat_api.py         # Chat endpoint
â”‚   â”‚   â”œâ”€â”€ health.py           # Health check endpoints
â”‚   â”œâ”€â”€ ml/                      # Machine learning
â”‚   â”‚   â”œâ”€â”€ train.py            # Model training
â”‚   â”‚   â”œâ”€â”€ predict.py          # Intent prediction
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Model evaluation
â”‚   â”‚   â”œâ”€â”€ retrain.py          # Retraining pipeline
â”‚   â”œâ”€â”€ nlp/                     # NLP processing
â”‚   â”‚   â”œâ”€â”€ preprocess.py       # Text cleaning
â”‚   â”‚   â”œâ”€â”€ tokenizer.py        # Tokenization
â”‚   â”‚   â”œâ”€â”€ lemmatizer.py       # Lemmatization
â”‚   â”œâ”€â”€ pipeline/                # Data processing
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # Load training data
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py  # TF-IDF vectorization
â”‚   â”‚   â”œâ”€â”€ response_selector.py    # Response selection
â”‚   â”‚   â”œâ”€â”€ confidence.py       # Confidence checking
â”‚   â”œâ”€â”€ knowledge_base/          # Knowledge base
â”‚   â”‚   â”œâ”€â”€ responses.py        # Intent to response mapping
â”‚   â”‚   â”œâ”€â”€ fallback.json       # Fallback responses
â”‚   â”œâ”€â”€ models/                  # Trained model artifacts
â”‚   â”‚   â”œâ”€â”€ intent_model.pkl
â”‚   â”‚   â”œâ”€â”€ vectorizer.pkl
â”‚   â”‚   â”œâ”€â”€ label_encoder.pkl
â”‚   â””â”€â”€ logs/                    # Logging
â”‚       â”œâ”€â”€ chat_logs.txt
â”‚       â””â”€â”€ low_confidence_queries.json
â”œâ”€â”€ frontend/                     # Streamlit UI
â”‚   â”œâ”€â”€ app.py                   # Main interface
â”‚   â”œâ”€â”€ ui_components.py         # Reusable components
â”œâ”€â”€ data/                         # Data files
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ scraped__data.json
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ _intents.json
â”‚   â””â”€â”€ scraper/
â”‚       â”œâ”€â”€ scrape_.py      # Web scraper
â”‚       â”œâ”€â”€ page_parser.py      # HTML parser
â”‚       â””â”€â”€ intent_mapper.py    # Intent mapping
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â”œâ”€â”€ test_nlp.py
â”‚   â”œâ”€â”€ test_ml.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ phase.md                     # Development phases
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Option 1: Run Everything (Automatic - Windows)
```bash
run.bat
```
This script automatically:
1. Scrapes  website data
2. Trains the ML model
3. Starts backend API (port 8000)
4. Launches Streamlit UI (port 8501)

### Option 2: Manual Startup

**Terminal 1 - Backend API:**
```bash
cd backend
python main.py
```
Or using Uvicorn directly:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```
API will be available at: `http://localhost:8000`
- Swagger Docs: `http://localhost:8000/docs`
- Health Check: `http://localhost:8000/health/status`

**Terminal 2 - Frontend UI:**
```bash
cd frontend
streamlit run app.py
```
UI will be available at: `http://localhost:8501`

### Option 3: Prepare Data and Train Model

**Scrape website:**
```bash
python data/scraper/scrape_.py
```

**Train ML model:**
```bash
python backend/ml/train.py
```

## ğŸ“Š API Endpoints

### Health Endpoints
- `GET /` - Welcome message
- `GET /health/status` - API status and DB statistics
- `GET /health/stats` - Chatbot performance stats
- `GET /health/config` - Current API configuration

### Chat Endpoints
- `POST /chat/ask` - Main chatbot endpoint
  ```json
  Request: {"message": "What courses do you offer?"}
  Response: {
    "user_query": "What courses do you offer?",
    "intent": "academics",
    "confidence": 0.87,
    "response": "Our academic programs are designed to meet international standards...",
    "source": "knowledge_base"
  }
  ```

- `GET /chat/logs` - Chat history (supports ?limit=50)
- `GET /chat/intents` - Available intents

## ğŸ§ª Testing

Run comprehensive test suite:
```bash
pytest tests/
```

Run specific test file:
```bash
pytest tests/test_nlp.py -v
```

Run with coverage:
```bash
pytest tests/ --cov=backend --cov=data
```

Test coverage includes:
- **Data Scraping** - Web scraping and parsing
- **NLP Pipeline** - Preprocessing, tokenization, lemmatization
- **ML Model** - Training, prediction, confidence scoring
- **API Endpoints** - Request validation, error handling, response format
- **Database** - Chat logging, statistics

## ğŸ” Monitoring & Analytics

### Chat Logs Database
SQLite database stores all interactions:
- Chat history with timestamps
- Predicted intents and confidence scores
- Response sources (Knowledge Base vs Fallback)
- Low-confidence queries for analysis

Access via API:
```bash
GET http://localhost:8000/chat/logs
```

### Log Files
- `backend/logs/chat_logs.txt` - Human-readable chat log
- `backend/logs/low_confidence_queries.json` - Queries for improvement

### Statistics
```bash
GET http://localhost:8000/health/stats
```
Returns:
- Total chats processed
- Average confidence score
- Low-confidence query count

## ğŸ“ Continuous Improvement

### Identify Low-Confidence Queries
```bash
GET http://localhost:8000/chat/logs?limit=100
# Filter responses with confidence < 0.5
```

### Evaluate Model Performance
```bash
python backend/ml/evaluator.py
```
Shows:
- Model accuracy
- Precision/recall per intent
- Confusion matrix

### Retrain Model with New Data
```bash
python backend/ml/retrain.py
```
Updates:
- Intent mappings from knowledge base
- Re-trains model with updated data
- Saves improved model artifacts

## ğŸ› ï¸ Troubleshooting

### Issue: "Models not loaded" error
**Solution:** Train the model first
```bash
python backend/ml/train.py
```

### Issue: "Backend not reachable" in UI
**Solution:** Ensure backend is running on localhost:8000
```bash
cd backend && python main.py
```

### Issue: Port 8000 already in use
**Solution:** Use different port
```bash
uvicorn main:app --port 8001
```
Update `frontend/app.py` API_BASE_URL accordingly.

### Issue: Dependencies installation fails
**Solution:** Upgrade pip and retry
```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

## ğŸ“ Development Phases

The project follows an 8-phase development roadmap:

1. âœ… **Phase 1** - Data Layer (Scraper & Data Preparation)
2. âœ… **Phase 2** - NLP Pipeline (Preprocessing, Tokenization, Lemmatization)
3. âœ… **Phase 3** - ML Model (Training & Prediction)
4. âœ… **Phase 4** - Knowledge Base & Response Selection
5. âœ… **Phase 5** - API & Backend Infrastructure
6. âœ… **Phase 6** - Frontend UI (Streamlit)
7. âœ… **Phase 7** - Testing, Requirements & Deployment
8. âœ… **Phase 8** - ML Operations (Evaluation & Retraining)

See `phase.md` for detailed implementation status.

## ğŸ¤ Contributing

To extend the chatbot:

1. **Add new intents:** Update `data/processed/_intents.json`
2. **Improve NLP:** Enhance `backend/nlp/` modules
3. **Retrain model:** Run `python backend/ml/retrain.py`
4. **Test changes:** Add tests to `tests/` directory
5. **Monitor performance:** Check `backend/logs/`

## ğŸ“„ License

This project is for educational purposes.

## ğŸ‘¨â€ğŸ’» Support

For issues or questions:
1. Check troubleshooting section above
2. Review logs in `backend/logs/`
3. Verify all dependencies are installed
4. Ensure Python 3.9+ is being used
